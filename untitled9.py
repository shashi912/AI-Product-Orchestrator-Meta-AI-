# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KSQ9K9ocNNKXP9y0xbcBrRi0ptpbgRn9
"""

# product_orchestrator.py
"""
ì¹˜AI Product Orchestrator (Meta-AI)
Complete example: synthetic dataset -> cleaning -> feature engineering -> Random Forest regression -> evaluation -> plots
"""

import os
import random
import numpy as np
import pandas as pd
from datetime import datetime, timedelta

# plotting
import matplotlib.pyplot as plt

# modeling
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import joblib

# reproducibility
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)

# ---------------------------
# 1) Generate synthetic dataset
# ---------------------------
def generate_synthetic_data(num_products=50, months=36, start_date="2022-01-01"):
    """
    Create synthetic monthly product-level data for `num_products` over `months`.
    Returns a DataFrame with columns:
    - date, product_id, price, marketing_spend, promo_flag, stock_level, category, region, sales (target)
    """
    start = pd.to_datetime(start_date)
    rows = []
    categories = ['Basic', 'Pro', 'Enterprise']
    regions = ['APAC', 'EMEA', 'AMER']

    for pid in range(1, num_products + 1):
        base_demand = np.random.uniform(50, 500)  # baseline monthly demand
        price = np.round(np.random.uniform(10, 200), 2)
        category = np.random.choice(categories, p=[0.6, 0.3, 0.1])
        region = np.random.choice(regions)
        trend = np.random.uniform(-0.01, 0.05)  # slight upward or downward trend
        seasonality_strength = np.random.uniform(0.05, 0.35)

        for m in range(months):
            date = start + pd.DateOffset(months=m)
            month = date.month
            # seasonality (e.g., peaks in month 11-12)
            seasonality = 1 + seasonality_strength * np.sin((2 * np.pi * (month - 1)) / 12)
            # promotions randomly
            promo_flag = np.random.binomial(1, 0.12)
            marketing_spend = 500 * promo_flag + np.random.normal(200, 80)
            # stock noise
            stock_level = max(0, np.random.normal(1000, 300))

            # price sensitivity: higher price -> lower demand
            price_effect = max(0.4, 1 - (price / 500.0))

            # demand formula (synthetic)
            base = base_demand * (1 + trend * m) * seasonality * price_effect
            promo_boost = base * (0.5 if promo_flag else 0.0)
            marketing_boost = base * (0.002 * marketing_spend)
            noise = np.random.normal(0, base * 0.1)

            sales = max(0, base + promo_boost + marketing_boost + noise)
            # Round sales to integer units
            sales_units = int(round(sales))

            rows.append({
                'date': date,
                'product_id': f'P{pid:03d}',
                'price': price,
                'marketing_spend': max(0, marketing_spend),
                'promo_flag': promo_flag,
                'stock_level': int(stock_level),
                'category': category,
                'region': region,
                'sales': sales_units
            })

    df = pd.DataFrame(rows)
    return df

# generate
df = generate_synthetic_data(num_products=80, months=36, start_date="2022-01-01")
print("Synthetic data rows:", len(df))
print(df.head())

# ---------------------------
# 2) Basic cleaning
# ---------------------------
def clean_data(df):
    df = df.copy()
    # ensure date is datetime
    df['date'] = pd.to_datetime(df['date'])
    # fill missing numerical columns with median if they exist
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    for c in num_cols:
        if df[c].isnull().any():
            df[c] = df[c].fillna(df[c].median())
    # categorical fill
    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    for c in cat_cols:
        if df[c].isnull().any():
            df[c] = df[c].fillna('Unknown')
    return df

df = clean_data(df)

# ---------------------------
# 3) Feature engineering
# ---------------------------
def engineer_features(df):
    df = df.copy()
    # temporal features
    df['year'] = df['date'].dt.year
    df['month'] = df['date'].dt.month
    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)

    # lag features: previous month sales per product
    df = df.sort_values(['product_id', 'date'])
    df['sales_lag_1'] = df.groupby('product_id')['sales'].shift(1).fillna(method='bfill')
    df['sales_lag_3'] = df.groupby('product_id')['sales'].shift(3).fillna(method='bfill')
    df['sales_roll_3'] = df.groupby('product_id')['sales'].rolling(window=3, min_periods=1).mean().reset_index(level=0, drop=True)

    # price_ratio: price relative to mean price of product
    df['mean_price_by_product'] = df.groupby('product_id')['price'].transform('mean')
    df['price_ratio'] = df['price'] / df['mean_price_by_product']

    # encode categorical as simple label encoding for tree-based model
    df['category_code'] = df['category'].astype('category').cat.codes
    df['region_code'] = df['region'].astype('category').cat.codes

    # drop columns not used for modeling
    modeling_cols = [
        'date','product_id','price','marketing_spend','promo_flag','stock_level',
        'category','region','sales',
        'year','month','month_sin','month_cos',
        'sales_lag_1','sales_lag_3','sales_roll_3',
        'price_ratio','category_code','region_code'
    ]
    df = df[modeling_cols]
    return df

df = engineer_features(df)
print("After feature engineering:", df.shape)
print(df.head())

# ---------------------------
# 4) Exploratory Data Analysis (simple)
# ---------------------------
def simple_eda(df, quick_plots=True):
    print("\n--- EDA Summary ---")
    print("Date range:", df['date'].min(), "to", df['date'].max())
    print("Products:", df['product_id'].nunique())
    print("Sales (summary):")
    print(df['sales'].describe())

    if quick_plots:
        # monthly total sales
        monthly = df.groupby('date')['sales'].sum().reset_index()
        plt.figure(figsize=(10,4))
        plt.plot(monthly['date'], monthly['sales'])
        plt.title('Total Monthly Sales (All Products)')
        plt.xlabel('Date')
        plt.ylabel('Sales units')
        plt.tight_layout()
        plt.show()

        # histogram of sales
        plt.figure(figsize=(6,4))
        plt.hist(df['sales'], bins=40)
        plt.title('Distribution of sales per product-month')
        plt.xlabel('Sales')
        plt.ylabel('Count')
        plt.tight_layout()
        plt.show()

simple_eda(df)

# ---------------------------
# 5) Modeling: prepare data for training
# ---------------------------
# target: next-month sales. We'll predict current row's sales using current features (or
# can shift target to predict next month; for simplicity, we predict 'sales' with current features).
model_df = df.copy()

# Drop rows with any NaNs in modeling features (should be few due to bfill)
model_df = model_df.dropna().reset_index(drop=True)

# features and target
FEATURES = [
    'price','marketing_spend','promo_flag','stock_level',
    'month','month_sin','month_cos',
    'sales_lag_1','sales_lag_3','sales_roll_3',
    'price_ratio','category_code','region_code'
]
TARGET = 'sales'

X = model_df[FEATURES]
y = model_df[TARGET]

# train/test split grouped by time to avoid lookahead bias: use last 20% time as test
dates_sorted = model_df['date'].sort_values().unique()
split_idx = int(len(dates_sorted) * 0.8)
split_date = dates_sorted[split_idx]
print("Train up to date (inclusive):", split_date)

train_mask = model_df['date'] <= split_date
X_train, X_test = X[train_mask], X[~train_mask]
y_train, y_test = y[train_mask], y[~train_mask]

print("Train rows:", X_train.shape[0], "Test rows:", X_test.shape[0])

# scale numeric features (not necessary for tree models but good practice for other models)
NUMERIC_FEATURES = ['price','marketing_spend','stock_level','month_sin','month_cos','sales_lag_1','sales_lag_3','sales_roll_3','price_ratio']
scaler = StandardScaler()
X_train_scaled = X_train.copy()
X_test_scaled = X_test.copy()
X_train_scaled[NUMERIC_FEATURES] = scaler.fit_transform(X_train[NUMERIC_FEATURES])
X_test_scaled[NUMERIC_FEATURES] = scaler.transform(X_test[NUMERIC_FEATURES])

# Save scaler
os.makedirs('artifacts', exist_ok=True)
joblib.dump(scaler, 'artifacts/scaler.joblib')

# ---------------------------
# 6) Train Random Forest regressor
# ---------------------------
rf = RandomForestRegressor(n_estimators=200, max_depth=12, random_state=RANDOM_SEED, n_jobs=-1)
rf.fit(X_train_scaled, y_train)
# save model
joblib.dump(rf, 'artifacts/random_forest_model.joblib')

# ---------------------------
# 7) Evaluate
# ---------------------------
def evaluate_model(model, X_test, y_test):
    preds = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, preds))
    r2 = r2_score(y_test, preds)
    return preds, rmse, r2

preds_rf, rmse_rf, r2_rf = evaluate_model(rf, X_test_scaled, y_test)
print(f"RandomForest RMSE: {rmse_rf:.2f}, R2: {r2_rf:.3f}")

# attach predictions for visualization
results = X_test.copy()
results['actual'] = y_test.values
results['predicted'] = preds_rf
results['date'] = model_df.loc[~train_mask, 'date'].values
results['product_id'] = model_df.loc[~train_mask, 'product_id'].values

# ---------------------------
# 8) Visualize actual vs predicted (global & sample product)
# ---------------------------
# Global time series (sum per month)
monthly_results = results.groupby('date').agg({'actual':'sum','predicted':'sum'}).reset_index()
plt.figure(figsize=(10,4))
plt.plot(monthly_results['date'], monthly_results['actual'], label='actual')
plt.plot(monthly_results['date'], monthly_results['predicted'], label='predicted', linestyle='--')
plt.title('Actual vs Predicted: Total Monthly Sales (Test Period)')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.legend()
plt.tight_layout()
plt.show()

# sample product-level plot
sample_pid = results['product_id'].unique()[0]
prod_df = results[results['product_id'] == sample_pid].sort_values('date')
plt.figure(figsize=(8,3))
plt.plot(prod_df['date'], prod_df['actual'], marker='o', label='actual')
plt.plot(prod_df['date'], prod_df['predicted'], marker='x', linestyle='--', label='predicted')
plt.title(f'Actual vs Predicted for {sample_pid}')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.legend()
plt.tight_layout()
plt.show()

# residuals histogram
residuals = results['actual'] - results['predicted']
plt.figure(figsize=(6,4))
plt.hist(residuals, bins=40)
plt.title('Residuals Distribution (actual - predicted)')
plt.xlabel('Residual')
plt.tight_layout()
plt.show()

# save results
results.to_csv('artifacts/test_results.csv', index=False)
print("Saved artifacts under ./artifacts/")

# ---------------------------
# 9) Optional: LSTM approach (high-level example)
# ---------------------------
# NOTE: This is a compact example and requires tensorflow. Use when you want sequence modeling.
lstm_example = """
# Optional LSTM (sequence model) idea:
# - For each product, create sliding windows of previous 3-6 months of features
# - Stack sequences and train an LSTM (Keras) to predict next-month sales.
# Example libs: from tensorflow.keras.models import Sequential
# This is left as an exercise due to length; RandomForest is robust for tabular data.
"""

print("Done. RandomForest model trained and artifacts saved.")